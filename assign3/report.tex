\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
% \usepackage{mwe}
% \usepackage{subfig}
\usepackage{cleveref}

\setlength{\parskip}{1em}


\newenvironment{question}[2][Question]{\begin{trivlist}
\kern10pt
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}


\begin{document}

\title{DD2424 Deep Learning in Data Science Assignment 3}
\author{Lin Chun Hung, chlin3@kth.se}

\maketitle

\section{Basic Part (Part 1)}
\begin{question}{i}
    I used the central difference method to calculate the numerical gradients
    with respect to all the network parameters and used it check against with the
    analytical gradients.

    I checked two networks which are of 2, 3, and 4 layers with and without batch normalization.

    I checked the maximum relative error which was mentioned in assignment and I
    used the numpy function \texttt{numpy.testing.assert\_allclose} to test if
    the gradients calculated analytically and numerically are closed element-wise.
    During this time, the RNN model was set to be calculated in double precision.

    For the maximum relative error, only the gradients of bias vectors went up to
    1. This is because the numerical and analytical gradients are zero vectors and
    therefore it is easily get the relative error to 1. These were verified by the
    test assertions.

    I also checked if the numerical and analytical arrays were equals.

    For the test assertion, consider the following equation:
    \begin{equation*}
        % absolute(a - b) <= (atol + rtol * absolute(b))
        |a - b| \leq (\texttt{atol} + \texttt{rtol} * |b|)
    \end{equation*}
    where \texttt{atol} and \texttt{rtol} are the tolerance parameters.
    In the assertion, I set \texttt{atol} to be 1e-5 and \texttt{rtol} to be 1e-6.

    In addition, I did the sanity checking to check if a 2-layer network
    (with 50 hidden nodes) can overfit
    a small training set. I found that two networks (with or without batch normalization)
    were able to overfit a small training set.

    The number of epochs was 200 and the initial learning rate was 0.05. The learning
    rate was halved every 20 epochs.

    With batch normalization, the loss after training was 0.244 and the training
    accuracy was 96.000%.
    Without batch normalization, the loss after training was 0.103 and the training
    accuracy was 98.990%.

    With these checking, I considered my analytical gradient calculations were bug free.
\end{question}


\begin{question}{ii}
    The evolution of the loss function of the 3-layer network with and without
    batch normalization is shown in \cref{plt:loss_3l}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth]{./loss_3l.png}
        \caption{The loss function of the 3-layer network}
        \label{plt:loss_3l}
    \end{figure}
\end{question}

\begin{question}{iii}
    The evolution of the loss function of the 6-layer network with and without
    batch normalization is shown in \cref{plt:loss_6l}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth]{./loss_6l.png}
        \caption{The loss function of the 6-layer network}
        \label{plt:loss_6l}
    \end{figure}
\end{question}

\begin{question}{iv}
    I did coarse-to-fine random search.
    For the coarse search, I did a random search on \texttt{lambda} in log scale.
    I drew 20 \texttt{lambda} values from the uniform distribution in log scale.
    The lower bound and the upper bound of the uniform distribution of $\log_{10}(\texttt{lambda})$
    were -5 and -1 respectivelly. I ran only 2 cycle and set the step size as
    \begin{align*}
        \texttt{n\_s} = 2 ~ \texttt{floor}(n / n\_batch)
    \end{align*}
    The coarse search result is plotted in \cref{plt:coarse_search}.
    I got the validation accuracy 52.28\% when \texttt{lambda} equals to 0.00115.

    For the fine search, I did the same but I changed the range of
    $\log_{10}(\texttt{lambda})$ to [-2, -4]. I changed it since
    \cref{plt:coarse_search} indicated that we can obtain a optimal performance
    when $\log_{10}(\texttt{lambda})$ in between -2 and -4.
    I also changed the step size as
    \begin{align*}
        \texttt{n\_s} = 3 ~ \texttt{floor}(n / n\_batch)
    \end{align*}

    The fine search result is plotted in \cref{plt:fine_search}.
    I got the validation accuracy 52.38\% when \texttt{lambda} equals to 0.00560.

    The test accuracy of the corresponding optimal \texttt{lambda} value was 52.89\%

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth]{./coarse_search.png}
        \caption{The coarse search result}
        \label{plt:coarse_search}
    \end{figure}

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.8\textwidth]{./fine_search.png}
        \caption{The fine search result}
        \label{plt:fine_search}
    \end{figure}
\end{question}

% Q5
\begin{question}{v}

    The required plots are shown in \cref{fig:all_sig}.

    \begin{figure}
        \begin{subfigure}{.5\linewidth}
        \centering
        \includegraphics[width=\textwidth]{1e-1.png}
        \caption{}
        \label{fig:sig_1e-1}
    \end{subfigure}%
    \begin{subfigure}{.5\linewidth}
        \centering
        \includegraphics[width=\textwidth]{1e-3.png}
        \caption{}
        \label{fig:sig_1e-3}
    \end{subfigure}\\[1ex]
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.5\textwidth]{1e-4.png}
        \caption{}
        \label{fig:sig_1e-4}
        \end{subfigure}
        \caption{
            Three sigmas values for initialization.
            \cref{fig:sig_1e-1} is for \texttt{sig} = \texttt{1e-1};
            \cref{fig:sig_1e-3} is for \texttt{sig} = \texttt{1e-3};
            \cref{fig:sig_1e-4} is for \texttt{sig} = \texttt{1e-4}
        }
        \label{fig:all_sig}
    \end{figure}

    Obviously, training with batch normalization is more stable and robust as
    we see in \cref{fig:sig_1e-3} or \cref{fig:sig_1e-4}. Without batch normalization,
    we cannot train the networks when the initial sigmas were \texttt{1e-3} and \texttt{1e-4}.

    With the batch normalization, we can train a network with the initialization weighting
    drawing from a standard normal with a small variance.
    It is usefull in He initialization scheme since the variance of the normal distribution
    of the weight matrix is very small when the number of incoming nodes are large.

    We can see that even without batch normalization, the network is trainable when the sigma value is
    \texttt{1e-1} (see \cref{fig:sig_1e-1})
    and thus training without batch normalization is sensitive to the sigma value.
\end{question}

\end{document}
