\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}

\setlength{\parskip}{1em}


\newenvironment{question}[2][Question]{\begin{trivlist}
\kern10pt
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}


\begin{document}

\title{DD2424 Deep Learning in Data Science Assignment 3}
\author{Lin Chun Hung, chlin3@kth.se}

\maketitle

\section{Basic Part (Part 1)}
\begin{question}{i}
    I used the central difference method to calculate the numerical gradients
    with respect to all the network parameters and used it check against with the
    analytical gradients.

    I checked two networks which are of 2, 3, and 4 layers with and without batch normalization.

    I checked the maximum relative error which was mentioned in assignment and I
    used the numpy function \texttt{numpy.testing.assert\_allclose} to test if
    the gradients calculated analytically and numerically are closed element-wise.
    During this time, the RNN model was set to be calculated in double precision.

    For the maximum relative error, only the gradients of bias vectors went up to
    1. This is because the numerical and analytical gradients are zero vectors and
    therefore it is easily get the relative error to 1. These were verified by the
    test assertions.

    I also checked if the numerical and analytical arrays were equals.

    For the test assertion, consider the following equation:
    \begin{equation*}
        % absolute(a - b) <= (atol + rtol * absolute(b))
        |a - b| \leq (\texttt{atol} + \texttt{rtol} * |b|)
    \end{equation*}
    where \texttt{atol} and \texttt{rtol} are the tolerance parameters.
    In the assertion, I set \texttt{atol} to be 1e-5 and \texttt{rtol} to be 1e-6.

    In addition, I did the sanity checking to check if a 2-layer network can overfit
    a small training set. I found that two networks (with or without batch normalization)
    were able to overfit a small training set.

    With these checking, I considered my analytical gradient calculations were bug free.
\end{question}


\begin{question}{ii}

\end{question}

\end{document}
