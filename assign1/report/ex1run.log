-------- TRAINING PARAMS --------
decay: 1.0
n_batch: 100
wgt_init: {'scheme': 'random', 'std': 0.01}
dtype: float32
shuffle_per_epoch: False
verbose: True
n_epochs: 40
lambda_: 0.0
eta: 0.1
-------- TRAINING PARAMS --------
Iteration 0: train_loss = 14.157216; valid_loss = 14.302402; lrate = 0.100000
Iteration 1: train_loss = 14.126386; valid_loss = 14.406553; lrate = 0.100000
Iteration 2: train_loss = 11.153157; valid_loss = 11.315677; lrate = 0.100000
Iteration 3: train_loss = 8.095329; valid_loss = 8.193227; lrate = 0.100000
Iteration 4: train_loss = 8.501655; valid_loss = 8.806273; lrate = 0.100000
Iteration 5: train_loss = 5.647453; valid_loss = 6.065902; lrate = 0.100000
Iteration 6: train_loss = 10.596531; valid_loss = 10.873856; lrate = 0.100000
Iteration 7: train_loss = 7.461518; valid_loss = 7.772939; lrate = 0.100000
Iteration 8: train_loss = 11.645210; valid_loss = 12.121705; lrate = 0.100000
Iteration 9: train_loss = 7.144300; valid_loss = 7.587190; lrate = 0.100000
Iteration 10: train_loss = 6.169150; valid_loss = 6.590248; lrate = 0.100000
Iteration 11: train_loss = 10.899370; valid_loss = 11.340278; lrate = 0.100000
Iteration 12: train_loss = 10.472362; valid_loss = 11.055209; lrate = 0.100000
Iteration 13: train_loss = 11.299847; valid_loss = 11.665968; lrate = 0.100000
Iteration 14: train_loss = 10.960653; valid_loss = 11.364478; lrate = 0.100000
Iteration 15: train_loss = 8.374764; valid_loss = 8.847754; lrate = 0.100000
Iteration 16: train_loss = 10.168341; valid_loss = 10.689027; lrate = 0.100000
Iteration 17: train_loss = 6.065500; valid_loss = 6.533449; lrate = 0.100000
Iteration 18: train_loss = 10.516237; valid_loss = 11.027087; lrate = 0.100000
Iteration 19: train_loss = 10.489800; valid_loss = 10.965122; lrate = 0.100000
Iteration 20: train_loss = 7.660917; valid_loss = 8.182183; lrate = 0.100000
Iteration 21: train_loss = 3.693818; valid_loss = 4.261920; lrate = 0.100000
Iteration 22: train_loss = 11.116437; valid_loss = 11.674147; lrate = 0.100000
Iteration 23: train_loss = 7.319601; valid_loss = 7.768955; lrate = 0.100000
Iteration 24: train_loss = 10.034736; valid_loss = 10.484425; lrate = 0.100000
Iteration 25: train_loss = 11.277980; valid_loss = 11.767135; lrate = 0.100000
Iteration 26: train_loss = 7.045627; valid_loss = 7.626422; lrate = 0.100000
Iteration 27: train_loss = 6.924863; valid_loss = 7.471143; lrate = 0.100000
Iteration 28: train_loss = 5.143355; valid_loss = 5.809885; lrate = 0.100000
Iteration 29: train_loss = 7.806882; valid_loss = 8.445750; lrate = 0.100000
Iteration 30: train_loss = 9.438337; valid_loss = 10.013153; lrate = 0.100000
Iteration 31: train_loss = 5.828821; valid_loss = 6.361976; lrate = 0.100000
Iteration 32: train_loss = 7.807534; valid_loss = 8.532446; lrate = 0.100000
Iteration 33: train_loss = 6.891127; valid_loss = 7.593369; lrate = 0.100000
Iteration 34: train_loss = 6.863257; valid_loss = 7.441416; lrate = 0.100000
Iteration 35: train_loss = 6.552296; valid_loss = 7.157980; lrate = 0.100000
Iteration 36: train_loss = 6.028633; valid_loss = 6.694083; lrate = 0.100000
Iteration 37: train_loss = 5.899206; valid_loss = 6.588703; lrate = 0.100000
Iteration 38: train_loss = 9.633846; valid_loss = 10.212400; lrate = 0.100000
Iteration 39: train_loss = 9.592562; valid_loss = 10.270117; lrate = 0.100000
Total used time =  5.287701368331909
Accuracy: 0.1832
===========================================================
-------- TRAINING PARAMS --------
decay: 1.0
n_batch: 100
wgt_init: {'scheme': 'random', 'std': 0.01}
dtype: float32
shuffle_per_epoch: False
verbose: True
n_epochs: 40
lambda_: 0.0
eta: 0.01
-------- TRAINING PARAMS --------
Iteration 0: train_loss = 2.020952; valid_loss = 2.043770; lrate = 0.010000
Iteration 1: train_loss = 1.941676; valid_loss = 1.974444; lrate = 0.010000
Iteration 2: train_loss = 1.898831; valid_loss = 1.939041; lrate = 0.010000
Iteration 3: train_loss = 1.870497; valid_loss = 1.916973; lrate = 0.010000
Iteration 4: train_loss = 1.849700; valid_loss = 1.901718; lrate = 0.010000
Iteration 5: train_loss = 1.833393; valid_loss = 1.890433; lrate = 0.010000
Iteration 6: train_loss = 1.820019; valid_loss = 1.881676; lrate = 0.010000
Iteration 7: train_loss = 1.808695; valid_loss = 1.874638; lrate = 0.010000
Iteration 8: train_loss = 1.798879; valid_loss = 1.868828; lrate = 0.010000
Iteration 9: train_loss = 1.790213; valid_loss = 1.863932; lrate = 0.010000
Iteration 10: train_loss = 1.782454; valid_loss = 1.859736; lrate = 0.010000
Iteration 11: train_loss = 1.775426; valid_loss = 1.856091; lrate = 0.010000
Iteration 12: train_loss = 1.769000; valid_loss = 1.852888; lrate = 0.010000
Iteration 13: train_loss = 1.763077; valid_loss = 1.850047; lrate = 0.010000
Iteration 14: train_loss = 1.757580; valid_loss = 1.847504; lrate = 0.010000
Iteration 15: train_loss = 1.752450; valid_loss = 1.845213; lrate = 0.010000
Iteration 16: train_loss = 1.747637; valid_loss = 1.843136; lrate = 0.010000
Iteration 17: train_loss = 1.743102; valid_loss = 1.841241; lrate = 0.010000
Iteration 18: train_loss = 1.738812; valid_loss = 1.839504; lrate = 0.010000
Iteration 19: train_loss = 1.734740; valid_loss = 1.837905; lrate = 0.010000
Iteration 20: train_loss = 1.730861; valid_loss = 1.836428; lrate = 0.010000
Iteration 21: train_loss = 1.727158; valid_loss = 1.835057; lrate = 0.010000
Iteration 22: train_loss = 1.723613; valid_loss = 1.833782; lrate = 0.010000
Iteration 23: train_loss = 1.720212; valid_loss = 1.832592; lrate = 0.010000
Iteration 24: train_loss = 1.716942; valid_loss = 1.831479; lrate = 0.010000
Iteration 25: train_loss = 1.713793; valid_loss = 1.830435; lrate = 0.010000
Iteration 26: train_loss = 1.710754; valid_loss = 1.829454; lrate = 0.010000
Iteration 27: train_loss = 1.707818; valid_loss = 1.828530; lrate = 0.010000
Iteration 28: train_loss = 1.704977; valid_loss = 1.827660; lrate = 0.010000
Iteration 29: train_loss = 1.702224; valid_loss = 1.826837; lrate = 0.010000
Iteration 30: train_loss = 1.699553; valid_loss = 1.826060; lrate = 0.010000
Iteration 31: train_loss = 1.696960; valid_loss = 1.825324; lrate = 0.010000
Iteration 32: train_loss = 1.694439; valid_loss = 1.824627; lrate = 0.010000
Iteration 33: train_loss = 1.691985; valid_loss = 1.823965; lrate = 0.010000
Iteration 34: train_loss = 1.689596; valid_loss = 1.823337; lrate = 0.010000
Iteration 35: train_loss = 1.687267; valid_loss = 1.822740; lrate = 0.010000
Iteration 36: train_loss = 1.684995; valid_loss = 1.822173; lrate = 0.010000
Iteration 37: train_loss = 1.682777; valid_loss = 1.821633; lrate = 0.010000
Iteration 38: train_loss = 1.680610; valid_loss = 1.821120; lrate = 0.010000
Iteration 39: train_loss = 1.678492; valid_loss = 1.820631; lrate = 0.010000
Total used time =  5.72936224937439
Accuracy: 0.3693
===========================================================
-------- TRAINING PARAMS --------
decay: 1.0
n_batch: 100
wgt_init: {'scheme': 'random', 'std': 0.01}
dtype: float32
shuffle_per_epoch: False
verbose: True
n_epochs: 40
lambda_: 0.1
eta: 0.01
-------- TRAINING PARAMS --------
Iteration 0: train_loss = 2.058826; valid_loss = 2.079977; lrate = 0.010000
Iteration 1: train_loss = 2.015046; valid_loss = 2.043476; lrate = 0.010000
Iteration 2: train_loss = 1.999812; valid_loss = 2.032648; lrate = 0.010000
Iteration 3: train_loss = 1.993238; valid_loss = 2.029086; lrate = 0.010000
Iteration 4: train_loss = 1.990022; valid_loss = 2.028035; lrate = 0.010000
Iteration 5: train_loss = 1.988295; valid_loss = 2.027905; lrate = 0.010000
Iteration 6: train_loss = 1.987294; valid_loss = 2.028100; lrate = 0.010000
Iteration 7: train_loss = 1.986666; valid_loss = 2.028380; lrate = 0.010000
Iteration 8: train_loss = 1.986242; valid_loss = 2.028650; lrate = 0.010000
Iteration 9: train_loss = 1.985931; valid_loss = 2.028873; lrate = 0.010000
Iteration 10: train_loss = 1.985686; valid_loss = 2.029042; lrate = 0.010000
Iteration 11: train_loss = 1.985478; valid_loss = 2.029156; lrate = 0.010000
Iteration 12: train_loss = 1.985292; valid_loss = 2.029224; lrate = 0.010000
Iteration 13: train_loss = 1.985120; valid_loss = 2.029251; lrate = 0.010000
Iteration 14: train_loss = 1.984957; valid_loss = 2.029244; lrate = 0.010000
Iteration 15: train_loss = 1.984800; valid_loss = 2.029211; lrate = 0.010000
Iteration 16: train_loss = 1.984646; valid_loss = 2.029156; lrate = 0.010000
Iteration 17: train_loss = 1.984495; valid_loss = 2.029083; lrate = 0.010000
Iteration 18: train_loss = 1.984346; valid_loss = 2.028998; lrate = 0.010000
Iteration 19: train_loss = 1.984199; valid_loss = 2.028902; lrate = 0.010000
Iteration 20: train_loss = 1.984053; valid_loss = 2.028797; lrate = 0.010000
Iteration 21: train_loss = 1.983909; valid_loss = 2.028687; lrate = 0.010000
Iteration 22: train_loss = 1.983766; valid_loss = 2.028572; lrate = 0.010000
Iteration 23: train_loss = 1.983624; valid_loss = 2.028453; lrate = 0.010000
Iteration 24: train_loss = 1.983484; valid_loss = 2.028332; lrate = 0.010000
Iteration 25: train_loss = 1.983345; valid_loss = 2.028209; lrate = 0.010000
Iteration 26: train_loss = 1.983207; valid_loss = 2.028085; lrate = 0.010000
Iteration 27: train_loss = 1.983070; valid_loss = 2.027960; lrate = 0.010000
Iteration 28: train_loss = 1.982935; valid_loss = 2.027835; lrate = 0.010000
Iteration 29: train_loss = 1.982801; valid_loss = 2.027709; lrate = 0.010000
Iteration 30: train_loss = 1.982668; valid_loss = 2.027584; lrate = 0.010000
Iteration 31: train_loss = 1.982536; valid_loss = 2.027460; lrate = 0.010000
Iteration 32: train_loss = 1.982405; valid_loss = 2.027336; lrate = 0.010000
Iteration 33: train_loss = 1.982276; valid_loss = 2.027212; lrate = 0.010000
Iteration 34: train_loss = 1.982148; valid_loss = 2.027090; lrate = 0.010000
Iteration 35: train_loss = 1.982021; valid_loss = 2.026968; lrate = 0.010000
Iteration 36: train_loss = 1.981895; valid_loss = 2.026848; lrate = 0.010000
Iteration 37: train_loss = 1.981771; valid_loss = 2.026727; lrate = 0.010000
Iteration 38: train_loss = 1.981647; valid_loss = 2.026609; lrate = 0.010000
Iteration 39: train_loss = 1.981525; valid_loss = 2.026491; lrate = 0.010000
Total used time =  5.043055295944214
Accuracy: 0.3339
===========================================================
-------- TRAINING PARAMS --------
decay: 1.0
n_batch: 100
wgt_init: {'scheme': 'random', 'std': 0.01}
dtype: float32
shuffle_per_epoch: False
verbose: True
n_epochs: 40
lambda_: 1.0
eta: 0.01
-------- TRAINING PARAMS --------
Iteration 0: train_loss = 2.205836; valid_loss = 2.218311; lrate = 0.010000
Iteration 1: train_loss = 2.205246; valid_loss = 2.218276; lrate = 0.010000
Iteration 2: train_loss = 2.204915; valid_loss = 2.218017; lrate = 0.010000
Iteration 3: train_loss = 2.204576; valid_loss = 2.217702; lrate = 0.010000
Iteration 4: train_loss = 2.204241; valid_loss = 2.217385; lrate = 0.010000
Iteration 5: train_loss = 2.203910; valid_loss = 2.217073; lrate = 0.010000
Iteration 6: train_loss = 2.203586; valid_loss = 2.216767; lrate = 0.010000
Iteration 7: train_loss = 2.203266; valid_loss = 2.216465; lrate = 0.010000
Iteration 8: train_loss = 2.202952; valid_loss = 2.216169; lrate = 0.010000
Iteration 9: train_loss = 2.202644; valid_loss = 2.215878; lrate = 0.010000
Iteration 10: train_loss = 2.202340; valid_loss = 2.215591; lrate = 0.010000
Iteration 11: train_loss = 2.202041; valid_loss = 2.215310; lrate = 0.010000
Iteration 12: train_loss = 2.201748; valid_loss = 2.215033; lrate = 0.010000
Iteration 13: train_loss = 2.201459; valid_loss = 2.214761; lrate = 0.010000
Iteration 14: train_loss = 2.201175; valid_loss = 2.214494; lrate = 0.010000
Iteration 15: train_loss = 2.200896; valid_loss = 2.214231; lrate = 0.010000
Iteration 16: train_loss = 2.200621; valid_loss = 2.213973; lrate = 0.010000
Iteration 17: train_loss = 2.200351; valid_loss = 2.213720; lrate = 0.010000
Iteration 18: train_loss = 2.200086; valid_loss = 2.213470; lrate = 0.010000
Iteration 19: train_loss = 2.199824; valid_loss = 2.213225; lrate = 0.010000
Iteration 20: train_loss = 2.199567; valid_loss = 2.212984; lrate = 0.010000
Iteration 21: train_loss = 2.199315; valid_loss = 2.212747; lrate = 0.010000
Iteration 22: train_loss = 2.199066; valid_loss = 2.212514; lrate = 0.010000
Iteration 23: train_loss = 2.198822; valid_loss = 2.212285; lrate = 0.010000
Iteration 24: train_loss = 2.198582; valid_loss = 2.212060; lrate = 0.010000
Iteration 25: train_loss = 2.198345; valid_loss = 2.211839; lrate = 0.010000
Iteration 26: train_loss = 2.198113; valid_loss = 2.211621; lrate = 0.010000
Iteration 27: train_loss = 2.197884; valid_loss = 2.211408; lrate = 0.010000
Iteration 28: train_loss = 2.197659; valid_loss = 2.211197; lrate = 0.010000
Iteration 29: train_loss = 2.197438; valid_loss = 2.210991; lrate = 0.010000
Iteration 30: train_loss = 2.197221; valid_loss = 2.210788; lrate = 0.010000
Iteration 31: train_loss = 2.197007; valid_loss = 2.210588; lrate = 0.010000
Iteration 32: train_loss = 2.196796; valid_loss = 2.210392; lrate = 0.010000
Iteration 33: train_loss = 2.196589; valid_loss = 2.210199; lrate = 0.010000
Iteration 34: train_loss = 2.196386; valid_loss = 2.210009; lrate = 0.010000
Iteration 35: train_loss = 2.196185; valid_loss = 2.209823; lrate = 0.010000
Iteration 36: train_loss = 2.195988; valid_loss = 2.209640; lrate = 0.010000
Iteration 37: train_loss = 2.195795; valid_loss = 2.209459; lrate = 0.010000
Iteration 38: train_loss = 2.195604; valid_loss = 2.209282; lrate = 0.010000
Iteration 39: train_loss = 2.195417; valid_loss = 2.209108; lrate = 0.010000
Total used time =  5.17729640007019
Accuracy: 0.2192
===========================================================
